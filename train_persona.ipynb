{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona Generation Training\n",
    "\n",
    "This notebook fine-tunes models to generate persona descriptions from dialogues.\n",
    "\n",
    "## Features\n",
    "- Model-agnostic: Supports T5, RuT5, BART, Qwen, Llama, and more\n",
    "- Auto-detects model architecture (seq2seq vs causal LM)\n",
    "- Train/validation/test split functionality\n",
    "- Evaluation with multiple metrics\n",
    "- Target speaker selection (1 or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets accelerate sacrebleu rouge-score tqdm sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up model and training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration - Change to any HuggingFace model\n",
    "# Seq2Seq models: cointegrated/rut5-base, google/flan-t5-base, facebook/bart-base\n",
    "# Causal LMs: Qwen/Qwen2.5-0.5B-Instruct, meta-llama/Llama-3.2-1B-Instruct\n",
    "MODEL_NAME = \"cointegrated/rut5-base\"\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"./data\"\n",
    "OUTPUT_DIR = \"./persona_model\"\n",
    "\n",
    "# Training configuration\n",
    "MAX_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 200\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "# Target speaker: 1 or 2\n",
    "TARGET_SPEAKER = 1\n",
    "\n",
    "# Data split: train, val, or test\n",
    "TRAIN_SPLIT = \"train\"\n",
    "VAL_SPLIT = \"val\"\n",
    "TEST_SPLIT = \"test\"\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Target Speaker: {TARGET_SPEAKER}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_dialogues(data_dir: str, split: str = \"train\") -> List[Dict]:\n",
    "    \"\"\"Load dialogues dataset from JSON file.\"\"\"\n",
    "    filename = f\"dialogues_{split}.json\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(data)} examples from {filename}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dialogue_text(dialogue: List[Dict], target_speaker: int) -> str:\n",
    "    \"\"\"Build dialogue text from messages.\"\"\"\n",
    "    messages = []\n",
    "    for msg in dialogue:\n",
    "        speaker_label = \"Пользователь 1\" if msg['speaker'] == 1 else \"Пользователь 2\"\n",
    "        messages.append(f\"{speaker_label}: {msg['text']}\")\n",
    "    return \"\\n\".join(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data: List[Dict], target_speaker: int, is_seq2seq: bool = False) -> List[Dict]:\n",
    "    \"\"\"Prepare data for fine-tuning.\"\"\"\n",
    "    prepared = []\n",
    "\n",
    "    for item in data:\n",
    "        dialogue = item.get(\"dialogue\", [])\n",
    "        \n",
    "        # Get target persona based on target_speaker\n",
    "        if target_speaker == 1:\n",
    "            reference_persona = item.get('persona_1', '')\n",
    "        else:\n",
    "            reference_persona = item.get('persona_2', '')\n",
    "\n",
    "        if not dialogue or not reference_persona:\n",
    "            continue\n",
    "\n",
    "        dialogue_text = build_dialogue_text(dialogue, target_speaker)\n",
    "        \n",
    "        if is_seq2seq:\n",
    "            prepared.append({\n",
    "                \"input\": dialogue_text,\n",
    "                \"target\": reference_persona\n",
    "            })\n",
    "        else:\n",
    "            # For causal LM, format as dialogue + instruction\n",
    "            formatted_text = f\"Диалог:\\n{dialogue_text}\\n\\nОпиши личность Пользователя {target_speaker}:\"\n",
    "            prepared.append({\"text\": formatted_text, \"target\": reference_persona})\n",
    "\n",
    "    print(f\"Prepared {len(prepared)} training examples\")\n",
    "    return prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_length, is_seq2seq: bool = False):\n",
    "    \"\"\"Tokenize text data.\"\"\"\n",
    "    if is_seq2seq:\n",
    "        inputs = tokenizer(\n",
    "            examples[\"input\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        targets = tokenizer(\n",
    "            examples[\"target\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        # For seq2seq, labels are the target ids\n",
    "        inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "        labels = [\n",
    "            [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]\n",
    "            for labels_seq in targets[\"input_ids\"]\n",
    "        ]\n",
    "        inputs[\"labels\"] = labels\n",
    "        return inputs\n",
    "    else:\n",
    "        # For causal LM, tokenize the full text\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and validation data\n",
    "print(\"Loading training data...\")\n",
    "train_data = load_dialogues(DATA_DIR, TRAIN_SPLIT)\n",
    "\n",
    "print(\"Loading validation data...\")\n",
    "val_data = load_dialogues(DATA_DIR, VAL_SPLIT)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}\")\n",
    "\n",
    "# Prepare training data\n",
    "train_prepared = prepare_training_data(train_data, TARGET_SPEAKER)\n",
    "val_prepared = prepare_training_data(val_data, TARGET_SPEAKER)\n",
    "\n",
    "print(f\"Prepared train: {len(train_prepared)}, val: {len(val_prepared)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"Load model and tokenizer. Auto-detects seq2seq vs causal LM.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Try seq2seq first, fallback to causal LM\n",
    "    try:\n",
    "        from transformers import AutoModelForSeq2SeqLM\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        is_seq2seq = True\n",
    "        print(\"Detected: Seq2Seq model (encoder-decoder)\")\n",
    "    except (OSError, ValueError, KeyError):\n",
    "        try:\n",
    "            from transformers import AutoModelForCausalLM\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            is_seq2seq = False\n",
    "            print(\"Detected: Causal LM (decoder-only)\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model {model_name}: {e}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer, is_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and auto-detect type\n",
    "model, tokenizer, is_seq2seq = load_model_and_tokenizer(MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_list(train_prepared)\n",
    "val_dataset = Dataset.from_list(val_prepared)\n",
    "\n",
    "# Determine columns to remove\n",
    "cols_to_remove = [\"input\", \"target\"] if is_seq2seq else [\"text\", \"target\"]\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer, MAX_LENGTH, is_seq2seq),\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    ")\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer, MAX_LENGTH, is_seq2seq),\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Trainer, Seq2SeqTrainer,\n",
    "    TrainingArguments, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "# Data collator and trainer class\n",
    "if is_seq2seq:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=True,\n",
    "    )\n",
    "    TrainerClass = Seq2SeqTrainer\n",
    "    TrainingArgsClass = Seq2SeqTrainingArguments\n",
    "else:\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "    TrainerClass = Trainer\n",
    "    TrainingArgsClass = TrainingArguments\n",
    "\n",
    "print(f\"Using {TrainerClass.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArgsClass(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=3,\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    predict_with_generate=True if is_seq2seq else False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = TrainerClass(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This will take some time depending on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Functions\n",
    "\n",
    "Functions to generate persona descriptions from dialogues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_persona(model, tokenizer, dialogue: List[Dict], target_speaker: int,\n",
    "                      is_seq2seq: bool, max_new_tokens: int = 150, temperature: float = 0.7):\n",
    "    \"\"\"Generate a persona description from dialogue.\"\"\"\n",
    "    \n",
    "    dialogue_text = build_dialogue_text(dialogue, target_speaker)\n",
    "    \n",
    "    if is_seq2seq:\n",
    "        prompt = dialogue_text\n",
    "    else:\n",
    "        prompt = f\"Диалог:\\n{dialogue_text}\\n\\nОпиши личность Пользователя {target_speaker}:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    if is_seq2seq:\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if f\"Опиши личность Пользователя {target_speaker}:\" in full_output:\n",
    "            result = full_output.split(f\"Опиши личность Пользователя {target_speaker}:\")[-1].strip()\n",
    "        else:\n",
    "            result = full_output\n",
    "    \n",
    "    # Clean up\n",
    "    result = result.split(\"\\n\\n\")[0].strip()\n",
    "    result = result.split(\"Диалог:\")[0].strip()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model for evaluation\n",
    "eval_model, eval_tokenizer, eval_is_seq2seq = load_model_and_tokenizer(OUTPUT_DIR)\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, item in enumerate(val_data[:5]):\n",
    "    # Get reference persona\n",
    "    reference = item.get(f\"persona_{TARGET_SPEAKER}\", \"\")\n",
    "    dialogue = item.get(\"dialogue\", [])\n",
    "    \n",
    "    # Generate prediction\n",
    "    predicted = generate_persona(\n",
    "        eval_model, eval_tokenizer, dialogue, TARGET_SPEAKER,\n",
    "        eval_is_seq2seq, max_new_tokens=150, temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Reference Persona {TARGET_SPEAKER}: {reference}\")\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 }
}
