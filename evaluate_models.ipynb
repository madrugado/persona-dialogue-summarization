{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona Generation Model Evaluation\n",
    "\n",
    "This notebook evaluates pre-trained models for persona generation from dialogues without any training.\n",
    "\n",
    "## Features\n",
    "- Evaluate any HuggingFace model (seq2seq or causal LM)\n",
    "- Generate persona descriptions for either speaker 1 or 2\n",
    "- Model-specific batch sizes based on memory requirements\n",
    "- Batched generation for efficient GPU utilization\n",
    "- GPU memory monitoring\n",
    "- Multiple metrics: BLEU, ROUGE, CHRF, LaBSE similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets sacremoses sentence-transformers sacrebleu rouge-score tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up model and evaluation parameters:</cell_type>markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DATA_DIR = \"./data\"\n",
    "OUTPUT_FILE = \"persona_evaluation_results.csv\"\n",
    "NUM_SAMPLES = None  # Use full dataset\n",
    "MAX_NEW_TOKENS = 150\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "# Model-specific batch sizes (adjust based on GPU memory)\n",
    "MODEL_BATCH_SIZES = {\n",
    "    \"cointegrated/rut5-base\": 256,\n",
    "    \"cointegrated/rut5-small\": 256,\n",
    "    \"google/flan-t5-base\": 128,\n",
    "    \"google/flan-t5-small\": 256,\n",
    "    \"google/flan-t5-large\": 64,\n",
    "    \"facebook/bart-base\": 128,\n",
    "    \"facebook/bart-large\": 32,\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\": 256,\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": 128,\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": 64,\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\": 32,\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": 256,\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": 128,\n",
    "    \"meta-llama/Llama-3.2-8B-Instruct\": 64,\n",
    "}\n",
    "\n",
    "def get_batch_size(model_name: str, default_size: int = 128) -> int:\n",
    "    \"\"\"Get batch size for model, with fallback to default.\"\"\"\n",
    "    return MODEL_BATCH_SIZES.get(model_name, default_size)\n",
    "\n",
    "# Get batch size for selected model\n",
    "BATCH_SIZE = get_batch_size(MODEL_NAME, 128)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Target Speaker: 2\")\n",
    "print(f\"Samples to evaluate: All\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Upload test dialogues JSON file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded filename\n",
    "uploaded_file = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded: {uploaded_file}\")\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Move uploaded file to data directory\n",
    "import shutil\n",
    "if uploaded_file.endswith('.json'):\n",
    "    dest_path = os.path.join(DATA_DIR, \"dialogues_test.json\")\n",
    "    shutil.move(uploaded_file, dest_path)\n",
    "    print(f\"Moved to: {dest_path}\")\n",
    "else:\n",
    "    print(\"Please upload a JSON file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GPU Memory Check\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    free = total_memory - allocated\n",
    "    \n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"Available: {free:.2f} GB\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_dialogues(data_dir: str) -> List[Dict]:\n",
    "    \"\"\"Load test dialogues dataset from JSON file.\"\"\"\n",
    "    filename = \"dialogues_test.json\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(data)} examples from {filename}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_subset(data: List[Dict], num_samples: int = None) -> List[Dict]:\n",
    "    if num_samples:\n",
    "        return data[:num_samples]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = load_dialogues(DATA_DIR)\n",
    "eval_data = get_eval_subset(dialogues, NUM_SAMPLES)\n",
    "print(f\"\\nEvaluating on {len(eval_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str):\n",
    "    from transformers import AutoTokenizer\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    try:\n",
    "        from transformers import AutoModelForSeq2SeqLM\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        is_seq2seq = True\n",
    "        print(\"Detected: Seq2Seq model\")\n",
    "    except (OSError, ValueError, KeyError):\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        is_seq2seq = False\n",
    "        print(\"Detected: Causal LM\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load model {model_name}: {e}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer, is_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, is_seq2seq = load_model_and_tokenizer(MODEL_NAME)\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dialogue_text(dialogue: List[Dict]) -> str:\n",
    "    \"\"\"Build dialogue text from messages.\"\"\"\n",
    "    messages = []\n",
    "    for msg in dialogue:\n",
    "        speaker_label = \"Пользователь 1\" if msg['speaker'] == 1 else \"Пользователь 2\"\n",
    "        messages.append(f\"{speaker_label}: {msg['text']}\")\n",
    "    return \"\\n\".join(messages)\n",
    "\n",
    "\n",
    "def generate_personas_batched(model, tokenizer, dialogues: List[Dict],\n",
    "                                   is_seq2seq: bool,\n",
    "                                   max_new_tokens: int = 150, temperature: float = 0.7):\n",
    "    \"\"\"Generate persona descriptions from dialogues in batches.\"\"\"\n",
    "    \n",
    "    # Few-shot examples in Russian\n",
    "    examples = \"\"\"Ты - ассистент для описания личности человека на основе диалога. Опиши личность указанного участника в виде списка фактов.\n",
    "\n",
    "Пример 1:\n",
    "Диалог:\n",
    "Пользователь 1: Привет! Работаю учителем.\n",
    "Пользователь 2: Привет! А какие предметы?\n",
    "Пользователь 1: Математику и физику.\n",
    "Пользователь 2: Круто! У меня собака.\n",
    "Пользователь 1: А у меня две дочки.\n",
    "\n",
    "Опиши личность Пользователя 1:\n",
    "- Работает учителем (математика и физика)\n",
    "- Есть две дочери\n",
    "\n",
    "Пример 2:\n",
    "Диалог:\n",
    "Пользователь 1: Привет! Люблю путешествовать.\n",
    "Пользователь 2: Куда ездил?\n",
    "Пользователь 1: В Турцию, в Египет.\n",
    "Пользователь 2: Я люблю готовить, я повар.\n",
    "Пользователь 1: У меня есть собака.\n",
    "Пользователь 2: У меня подруга подарила котенка.\n",
    "\n",
    "Опиши личность Пользователя 2:\n",
    "- Любит готовить\n",
    "- Работает поваром\n",
    "- Есть подруга\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    if is_seq2seq:\n",
    "        # Seq2Seq: dialogue -> persona for speaker 2\n",
    "        prompts = [f\"{examples}Диалог:\\n{build_dialogue_text(d)}\\n\\nОпиши личность Пользователя 2:\" for d in dialogues]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    else:\n",
    "        # Causal LM\n",
    "        prompts = [f\"{examples}Диалог:\\n{build_dialogue_text(d)}\\n\\nОпиши личность Пользователя 2:\" for d in dialogues]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        full_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # Extract continuation after the prompt\n",
    "        results = []\n",
    "        for output in full_outputs:\n",
    "            # Find where our persona description starts\n",
    "            if \"Опиши личность Пользователя 2:\" in output:\n",
    "                result = output.split(\"Опиши личность Пользователя 2:\")[-1].strip()\n",
    "            else:\n",
    "                result = output\n",
    "            # Clean up\n",
    "            result = result.split(\"\\n\\n\")[0].strip()\n",
    "            result = result.split(\"Диалог:\")[0].strip()\n",
    "            results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Evaluation\n",
    "\n",
    "Test model on a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample predictions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get sample data\n",
    "sample_data = eval_data[:5]\n",
    "\n",
    "# Get reference personas (always persona_2)\n",
    "sample_refs = [\n",
    "    item.get(\"persona_2\", \"\") for item in sample_data\n",
    "]\n",
    "\n",
    "# Generate in batch\n",
    "predictions = generate_personas_batched(\n",
    "    model, tokenizer, sample_data, is_seq2seq, MAX_NEW_TOKENS, TEMPERATURE\n",
    ")\n",
    "\n",
    "for i, (item, predicted) in enumerate(zip(sample_data, predictions)):\n",
    "    reference = item.get(\"persona_2\", \"\")\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Reference Persona 2: {reference}\")\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Functions to compute metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# LaBSE Model Caching\n",
    "labse_model = None\n",
    "\n",
    "def get_labse_model():\n",
    "    global labse_model\n",
    "    if labse_model is None:\n",
    "        print(\"Loading LaBSE model...\")\n",
    "        labse_model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "    return labse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(references, hypotheses):\n",
    "    from sacrebleu.metrics import BLEU, CHRF\n",
    "    from rouge_score import rouge_scorer\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "    print(\"Computing BLEU...\")\n",
    "    bleu_metric = BLEU()\n",
    "    bleu_result = bleu_metric.corpus_score(hypotheses, [references])\n",
    "\n",
    "    print(\"Computing ROUGE...\")\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(\n",
    "        ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True\n",
    "    )\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores = rouge_scorer_instance.score(ref, hyp)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    print(\"Computing CHRF...\")\n",
    "    chrf_metric = CHRF()\n",
    "    chrf_result = chrf_metric.corpus_score(hypotheses, [references])\n",
    "\n",
    "    # LaBSE similarity - use cached model\n",
    "    labse = get_labse_model()\n",
    "    ref_embeddings = labse.encode(references, convert_to_tensor=True)\n",
    "    hyp_embeddings = labse.encode(hypotheses, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(hyp_embeddings, ref_embeddings)\n",
    "    similarity_scores = torch.diagonal(similarities).cpu().numpy()\n",
    "\n",
    "    return {\n",
    "        'BLEU': bleu_result.score,\n",
    "        'ROUGE-1': np.mean(rouge1_scores) * 100,\n",
    "        'ROUGE-2': np.mean(rouge2_scores) * 100,\n",
    "        'ROUGE-L': np.mean(rougeL_scores) * 100,\n",
    "        'CHRF': chrf_result.score,\n",
    "        'LaBSE-Similarity': np.mean(similarity_scores) * 100,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_batched(model, tokenizer, eval_data, is_seq2seq,\n",
    "                                   batch_size, max_new_tokens, temperature, output_file=None):\n",
    "    references, hypotheses, predictions = [], [], []\n",
    "    num_samples = len(eval_data)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "    print(f\"\\nEvaluating {num_samples} samples in {num_batches} batches (size={batch_size})...\")\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_data = eval_data[start_idx:end_idx]\n",
    "\n",
    "        # Get reference personas (always persona_2)\n",
    "        batch_refs = [item.get(\"persona_2\", \"\") for item in batch_data]\n",
    "\n",
    "        # Generate in batch\n",
    "        batch_preds = generate_personas_batched(\n",
    "            model, tokenizer, batch_data, is_seq2seq, max_new_tokens, temperature\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        for item, ref, pred in zip(batch_data, batch_refs, batch_preds):\n",
    "            references.append(ref)\n",
    "            hypotheses.append(pred)\n",
    "            predictions.append({\n",
    "                'id': item.get('id', ''),\n",
    "                'target_speaker': 2,\n",
    "                'reference': ref,\n",
    "                'predicted': pred,\n",
    "            })\n",
    "\n",
    "        print(f\"Processed batch {batch_idx + 1}/{num_batches} ({end_idx}/{num_samples} samples)\")\n",
    "\n",
    "    print(\"Computing metrics...\")\n",
    "    metrics = compute_metrics(references, hypotheses)\n",
    "\n",
    "    if output_file:\n",
    "        # Save as CSV\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(predictions)\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"\\nPredictions saved to {output_file}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "Download the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Target Speaker: 2\")\n",
    "print(f\"Test samples: {len(eval_data)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(\"\\n--- Metrics ---\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Multiple Models\n",
    "\n",
    "Define a list of models to compare on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_TO_COMPARE = [\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen3-0.6B\",\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "]\n",
    "\n",
    "COMPARE_NUM_SAMPLES = None  # Use full dataset\n",
    "\n",
    "compare_data = load_dialogues(DATA_DIR)\n",
    "compare_data = get_eval_subset(compare_data, COMPARE_NUM_SAMPLES)\n",
    "print(f\"\\nComparing {len(MODELS_TO_COMPARE)} models on {len(compare_data)} test samples...\")\n",
    "all_results = []\n",
    "\n",
    "for model_name in MODELS_TO_COMPARE:\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    try:\n",
    "        compare_model, compare_tokenizer, compare_is_seq2seq = load_model_and_tokenizer(model_name)\n",
    "        compare_batch_size = get_batch_size(model_name, 128)\n",
    "        compare_metrics = run_evaluation_batched(\n",
    "            compare_model, compare_tokenizer, compare_data, compare_is_seq2seq,\n",
    "            compare_batch_size, MAX_NEW_TOKENS, TEMPERATURE, None\n",
    "        )\n",
    "        result = {\n",
    "            \"model\": model_name,\n",
    "            \"target_speaker\": 2,\n",
    "            \"samples\": len(compare_data)\n",
    "        }\n",
    "        result.update(compare_metrics)\n",
    "        all_results.append(result)\n",
    "        del compare_model, compare_tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(all_results)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(df_results.to_string(index=False))\n",
    "df_results.to_csv('model_comparison.csv', index=False)\n",
    "print(\"\\nResults saved to model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting download...\")\n",
    "from google.colab import files\n",
    "files.download(OUTPUT_FILE)\n",
    "if os.path.exists('persona_model_comparison.csv'):\n",
    "    files.download('persona_model_comparison.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
