{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Persona Generation Model Evaluation\n\nThis notebook evaluates pre-trained models for persona generation from dialogues without any training.\n\n## Features\n- Evaluate any HuggingFace model (seq2seq or causal LM)\n- Generate persona descriptions for either speaker 1 or 2\n- Model-specific batch sizes based on memory requirements\n- Batched generation for efficient GPU utilization\n- GPU memory monitoring\n- Multiple metrics: BLEU, ROUGE, CHRF, LaBSE similarity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch transformers datasets sacremoses sentence-transformers sacrebleu rouge-score tqdm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\nSet up model and evaluation parameters:</cell_type>markdown"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\nDATA_DIR = \"./data\"\nOUTPUT_FILE = \"persona_evaluation_results.json\"\nNUM_SAMPLES = None  # Use full dataset\nMAX_NEW_TOKENS = 150\nTEMPERATURE = 0.7\n\n# Model-specific batch sizes (adjust based on GPU memory)\nMODEL_BATCH_SIZES = {\n    \"cointegrated/rut5-base\": 256,\n    \"cointegrated/rut5-small\": 256,\n    \"google/flan-t5-base\": 128,\n    \"google/flan-t5-small\": 256,\n    \"google/flan-t5-large\": 64,\n    \"facebook/bart-base\": 128,\n    \"facebook/bart-large\": 32,\n    \"Qwen/Qwen2.5-0.5B-Instruct\": 256,\n    \"Qwen/Qwen2.5-1.5B-Instruct\": 128,\n    \"Qwen/Qwen2.5-3B-Instruct\": 64,\n    \"Qwen/Qwen2.5-7B-Instruct\": 32,\n    \"meta-llama/Llama-3.2-1B-Instruct\": 256,\n    \"meta-llama/Llama-3.2-3B-Instruct\": 128,\n    \"meta-llama/Llama-3.2-8B-Instruct\": 64,\n}\n\ndef get_batch_size(model_name: str, default_size: int = 128) -> int:\n    \"\"\"Get batch size for model, with fallback to default.\"\"\"\n    return MODEL_BATCH_SIZES.get(model_name, default_size)\n\n# Get batch size for selected model\nBATCH_SIZE = get_batch_size(MODEL_NAME, 128)\n\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Target Speaker: 2\")\nprint(f\"Samples to evaluate: All\")\nprint(f\"Batch size: {BATCH_SIZE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nimport os\n\nprint(\"Upload test dialogues JSON file:\")\nuploaded = files.upload()\n\n# Get the uploaded filename\nuploaded_file = list(uploaded.keys())[0]\nprint(f\"Uploaded: {uploaded_file}\")\n\n# Create data directory if it doesn't exist\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Move uploaded file to data directory\nimport shutil\nif uploaded_file.endswith('.json'):\n    dest_path = os.path.join(DATA_DIR, \"dialogues_test.json\")\n    shutil.move(uploaded_file, dest_path)\n    print(f\"Moved to: {dest_path}\")\nelse:\n    print(\"Please upload a JSON file\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## GPU Memory Check"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GPU Memory Check\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    free = total_memory - allocated\n",
    "    \n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"Available: {free:.2f} GB\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Loading Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nfrom typing import List, Dict\n\ndef load_dialogues(data_dir: str) -> List[Dict]:\n    \"\"\"Load test dialogues dataset from JSON file.\"\"\"\n    filename = \"dialogues_test.json\"\n    filepath = os.path.join(data_dir, filename)\n\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    print(f\"Loaded {len(data)} examples from {filename}\")\n    return data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_eval_subset(data: List[Dict], num_samples: int = None) -> List[Dict]:\n    if num_samples:\n        return data[:num_samples]\n    return data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "dialogues = load_dialogues(DATA_DIR)\neval_data = get_eval_subset(dialogues, NUM_SAMPLES)\nprint(f\"\\nEvaluating on {len(eval_data)} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Loading Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\ndef load_model_and_tokenizer(model_name: str):\n    from transformers import AutoTokenizer\n    print(f\"Loading model: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    try:\n        from transformers import AutoModelForSeq2SeqLM\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n        )\n        is_seq2seq = True\n        print(\"Detected: Seq2Seq model\")\n    except (OSError, ValueError, KeyError):\n        from transformers import AutoModelForCausalLM\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n        )\n        is_seq2seq = False\n        print(\"Detected: Causal LM\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load model {model_name}: {e}\")\n\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    return model, tokenizer, is_seq2seq"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model, tokenizer, is_seq2seq = load_model_and_tokenizer(MODEL_NAME)\nprint(f\"Model loaded on: {model.device}\")\nprint(f\"Model parameters: {model.num_parameters():,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Batched Generation Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_dialogue_text(dialogue: List[Dict]) -> str:\n    \"\"\"Build dialogue text from messages.\"\"\"\n    messages = []\n    for msg in dialogue:\n        speaker_label = \"Пользователь 1\" if msg['speaker'] == 1 else \"Пользователь 2\"\n        messages.append(f\"{speaker_label}: {msg['text']}\")\n    return \"\\n\".join(messages)\n\n\ndef generate_personas_batched(model, tokenizer, dialogues: List[Dict],\n                                   is_seq2seq: bool,\n                                   max_new_tokens: int = 150, temperature: float = 0.7):\n    \"\"\"Generate persona descriptions from dialogues in batches.\"\"\"\n    \n    # Few-shot examples in Russian\n    examples = \"\"\"Ты - ассистент для описания личности человека на основе диалога. Опиши личность указанного участника в виде списка фактов.\n\nПример 1:\nДиалог:\nПользователь 1: Привет! Работаю учителем.\nПользователь 2: Привет! А какие предметы?\nПользователь 1: Математику и физику.\nПользователь 2: Круто! У меня собака.\nПользователь 1: А у меня две дочки.\n\nОпиши личность Пользователя 1:\n- Работает учителем (математика и физика)\n- Есть две дочери\n\nПример 2:\nДиалог:\nПользователь 1: Привет! Люблю путешествовать.\nПользователь 2: Куда ездил?\nПользователь 1: В Турцию, в Египет.\nПользователь 2: Я люблю готовить, я повар.\nПользователь 1: У меня есть собака.\nПользователь 2: У меня подруга подарила котенка.\n\nОпиши личность Пользователя 2:\n- Любит готовить\n- Работает поваром\n- Есть подруга\n\n\"\"\"\n    \n    if is_seq2seq:\n        # Seq2Seq: dialogue -> persona for speaker 2\n        prompts = [f\"{examples}Диалог:\\n{build_dialogue_text(d)}\\n\\nОпиши личность Пользователя 2:\" for d in dialogues]\n        inputs = tokenizer(prompts, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n        results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    else:\n        # Causal LM\n        prompts = [f\"{examples}Диалог:\\n{build_dialogue_text(d)}\\n\\nОпиши личность Пользователя 2:\" for d in dialogues]\n        inputs = tokenizer(prompts, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n        full_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        # Extract continuation after the prompt\n        results = []\n        for output in full_outputs:\n            # Find where our persona description starts\n            if \"Опиши личность Пользователя 2:\" in output:\n                result = output.split(\"Опиши личность Пользователя 2:\")[-1].strip()\n            else:\n                result = output\n            # Clean up\n            result = result.split(\"\\n\\n\")[0].strip()\n            result = result.split(\"Диалог:\")[0].strip()\n            results.append(result)\n    \n    return results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Quick Evaluation\n\nTest model on a few samples:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSample predictions:\")\nprint(\"=\" * 80)\n\n# Get sample data\nsample_data = eval_data[:5]\n\n# Get reference personas (always persona_2)\nsample_refs = [\n    item.get(\"persona_2\", \"\") for item in sample_data\n]\n\n# Generate in batch\npredictions = generate_personas_batched(\n    model, tokenizer, sample_data, is_seq2seq, MAX_NEW_TOKENS, TEMPERATURE\n)\n\nfor i, (item, predicted) in enumerate(zip(sample_data, predictions)):\n    reference = item.get(\"persona_2\", \"\")\n    print(f\"\\n--- Sample {i+1} ---\")\n    print(f\"Reference Persona 2: {reference}\")\n    print(f\"Predicted: {predicted}\")\n    print(\"-\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Evaluation Functions\n\nFunctions to compute metrics:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\n\nfrom sentence_transformers import SentenceTransformer, util\n\n# LaBSE Model Caching\nlabse_model = None\n\ndef get_labse_model():\n    global labse_model\n    if labse_model is None:\n        print(\"Loading LaBSE model...\")\n        labse_model = SentenceTransformer('sentence-transformers/LaBSE')\n    return labse_model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_metrics(references, hypotheses):\n    from sacrebleu.metrics import BLEU, CHRF\n    from rouge_score import rouge_scorer\n    from sentence_transformers import SentenceTransformer, util\n\n    print(\"Computing BLEU...\")\n    bleu_metric = BLEU()\n    bleu_result = bleu_metric.corpus_score(hypotheses, [references])\n\n    print(\"Computing ROUGE...\")\n    rouge_scorer_instance = rouge_scorer.RougeScorer(\n        ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True\n    )\n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n    for ref, hyp in zip(references, hypotheses):\n        scores = rouge_scorer_instance.score(ref, hyp)\n        rouge1_scores.append(scores['rouge1'].fmeasure)\n        rouge2_scores.append(scores['rouge2'].fmeasure)\n        rougeL_scores.append(scores['rougeL'].fmeasure)\n\n    print(\"Computing CHRF...\")\n    chrf_metric = CHRF()\n    chrf_result = chrf_metric.corpus_score(hypotheses, [references])\n\n    # LaBSE similarity - use cached model\n    labse = get_labse_model()\n    ref_embeddings = labse.encode(references, convert_to_tensor=True)\n    hyp_embeddings = labse.encode(hypotheses, convert_to_tensor=True)\n    similarities = util.cos_sim(hyp_embeddings, ref_embeddings)\n    similarity_scores = torch.diagonal(similarities).cpu().numpy()\n\n    return {\n        'BLEU': bleu_result.score,\n        'ROUGE-1': np.mean(rouge1_scores) * 100,\n        'ROUGE-2': np.mean(rouge2_scores) * 100,\n        'ROUGE-L': np.mean(rougeL_scores) * 100,\n        'CHRF': chrf_result.score,\n        'LaBSE-Similarity': np.mean(similarity_scores) * 100,\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_evaluation_batched(model, tokenizer, eval_data, is_seq2seq,\n                                   batch_size, max_new_tokens, temperature, output_file=None):\n    references, hypotheses, predictions = [], [], []\n    num_samples = len(eval_data)\n    num_batches = (num_samples + batch_size - 1) // batch_size\n    print(f\"\\nEvaluating {num_samples} samples in {num_batches} batches (size={batch_size})...\")\n\n    for batch_idx in range(num_batches):\n        start_idx = batch_idx * batch_size\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_data = eval_data[start_idx:end_idx]\n\n        # Get reference personas (always persona_2)\n        batch_refs = [item.get(\"persona_2\", \"\") for item in batch_data]\n\n        # Generate in batch\n        batch_preds = generate_personas_batched(\n            model, tokenizer, batch_data, is_seq2seq, max_new_tokens, temperature\n        )\n\n        # Store results\n        for item, ref, pred in zip(batch_data, batch_refs, batch_preds):\n            references.append(ref)\n            hypotheses.append(pred)\n            predictions.append({\n                'id': item.get('id', ''),\n                'target_speaker': 2,\n                'reference': ref,\n                'predicted': pred,\n            })\n\n        print(f\"Processed batch {batch_idx + 1}/{num_batches} ({end_idx}/{num_samples} samples)\")\n\n    print(\"Computing metrics...\")\n    metrics = compute_metrics(references, hypotheses)\n\n    if output_file:\n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(predictions, f, indent=2, ensure_ascii=False)\n        print(f\"\\nPredictions saved to {output_file}\")\n\n    return metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Download Results\n\nDownload the evaluation results:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\" * 80)\nprint(f\"\\nModel: {MODEL_NAME}\")\nprint(f\"Target Speaker: 2\")\nprint(f\"Test samples: {len(eval_data)}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(\"\\n--- Metrics ---\")\nfor key, value in metrics.items():\n    print(f\"{key}: {value:.4f}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Compare Multiple Models\n\nDefine a list of models to compare on test set:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MODELS_TO_COMPARE = [\n    \"Qwen/Qwen2.5-0.5B-Instruct\",\n    \"Qwen/Qwen2.5-1.5B-Instruct\",\n    \"meta-llama/Llama-3.2-1B-Instruct\",\n]\n\nCOMPARE_NUM_SAMPLES = None  # Use full dataset\n\ncompare_data = load_dialogues(DATA_DIR)\ncompare_data = get_eval_subset(compare_data, COMPARE_NUM_SAMPLES)\nprint(f\"\\nComparing {len(MODELS_TO_COMPARE)} models on {len(compare_data)} test samples...\")\nall_results = []\n\nfor model_name in MODELS_TO_COMPARE:\n    print(f\"\\n\" + \"=\" * 80)\n    print(f\"Evaluating: {model_name}\")\n    print(\"=\" * 80)\n    try:\n        compare_model, compare_tokenizer, compare_is_seq2seq = load_model_and_tokenizer(model_name)\n        compare_batch_size = get_batch_size(model_name, 128)\n        compare_metrics = run_evaluation_batched(\n            compare_model, compare_tokenizer, compare_data, compare_is_seq2seq,\n            compare_batch_size, MAX_NEW_TOKENS, TEMPERATURE, None\n        )\n        result = {\n            \"model\": model_name,\n            \"target_speaker\": 2,\n            \"samples\": len(compare_data)\n        }\n        result.update(compare_metrics)\n        all_results.append(result)\n        del compare_model, compare_tokenizer\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"Error evaluating {model_name}: {e}\")\n        continue"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import pandas as pd\n\ndf_results = pd.DataFrame(all_results)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MODEL COMPARISON RESULTS\")\nprint(\"=\" * 80)\nprint(df_results.to_string(index=False))\ndf_results.to_csv('persona_model_comparison.csv', index=False)\nprint(\"\\nResults saved to persona_model_comparison.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\ndf_results = pd.DataFrame(all_results)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MODEL COMPARISON RESULTS\")\nprint(\"=\" * 80)\nprint(df_results.to_string(index=False))\ndf_results.to_csv('model_comparison.csv', index=False)\nprint(\"\\nResults saved to model_comparison.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nStarting download...\")\nfrom google.colab import files\nfiles.download(OUTPUT_FILE)\nif os.path.exists('persona_model_comparison.csv'):\n    files.download('persona_model_comparison.csv')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}